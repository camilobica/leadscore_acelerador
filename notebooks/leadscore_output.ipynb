{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "060efa69",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [2]</a>'.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f1f891e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T12:02:06.916074Z",
     "iopub.status.busy": "2025-05-09T12:02:06.914052Z",
     "iopub.status.idle": "2025-05-09T12:02:13.053582Z",
     "shell.execute_reply": "2025-05-09T12:02:13.053582Z"
    },
    "papermill": {
     "duration": 6.165475,
     "end_time": "2025-05-09T12:02:13.053582",
     "exception": false,
     "start_time": "2025-05-09T12:02:06.888107",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === Bibliotecas padrão ===\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "\n",
    "# === Bibliotecas de terceiros ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cycler import cycler\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report,\n",
    "    mean_absolute_error, mean_squared_error, r2_score,\n",
    "    precision_recall_fscore_support, roc_auc_score, roc_curve\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.utils import compute_sample_weight\n",
    "\n",
    "import joblib\n",
    "\n",
    "# === Módulos locais ===\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Caminho absoluto até a pasta src\n",
    "src_path = r\"C:\\Users\\Camilo_Bica\\data_science\\consultoria\\acelerador_petrobras\\notebooks\\src\"\n",
    "\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "    \n",
    "# Garante que a pasta atual (notebooks/) esteja no sys.path\n",
    "notebooks_dir = os.path.abspath(os.getcwd())\n",
    "if notebooks_dir not in sys.path:\n",
    "    sys.path.append(notebooks_dir)\n",
    "\n",
    "# Agora importa diretamente SEM prefixo\n",
    "import leadscore_plot\n",
    "importlib.reload(leadscore_plot)\n",
    "\n",
    "from leadscore_plot import (\n",
    "    plot_comparativo_leads_alunos,\n",
    "    plot_histograma_leadscore,\n",
    "    plot_probabilidade_conversao_vs_score\n",
    ")\n",
    "\n",
    "# === Configuração de visualização ===\n",
    "cores = plt.get_cmap('Accent').colors\n",
    "ciclo_cores = cycler('color', cores)\n",
    "plt.rc('axes', prop_cycle=ciclo_cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c31e6d",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4940b997-10c8-4b0e-9d2f-2274c97a7c47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-09T12:02:13.095831Z",
     "iopub.status.busy": "2025-05-09T12:02:13.095831Z",
     "iopub.status.idle": "2025-05-09T12:02:19.773195Z",
     "shell.execute_reply": "2025-05-09T12:02:19.771181Z"
    },
    "papermill": {
     "duration": 6.695131,
     "end_time": "2025-05-09T12:02:19.775207",
     "exception": true,
     "start_time": "2025-05-09T12:02:13.080076",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TransportError",
     "evalue": "HTTPSConnectionPool(host='oauth2.googleapis.com', port=443): Max retries exceeded with url: /token (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000260070CA930>: Failed to resolve 'oauth2.googleapis.com' ([Errno 11001] getaddrinfo failed)\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:199\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 199\u001b[0m     sock \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[0;32m    200\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport),\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m    202\u001b[0m         source_address\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_address,\n\u001b[0;32m    203\u001b[0m         socket_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket_options,\n\u001b[0;32m    204\u001b[0m     )\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\connection.py:60\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgetaddrinfo(host, port, family, socket\u001b[38;5;241m.\u001b[39mSOCK_STREAM):\n\u001b[0;32m     61\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\socket.py:963\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    962\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 963\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m _socket\u001b[38;5;241m.\u001b[39mgetaddrinfo(host, port, family, \u001b[38;5;28mtype\u001b[39m, proto, flags):\n\u001b[0;32m    964\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    790\u001b[0m     conn,\n\u001b[0;32m    791\u001b[0m     method,\n\u001b[0;32m    792\u001b[0m     url,\n\u001b[0;32m    793\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    794\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    795\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    796\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    797\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    798\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    799\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    800\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    802\u001b[0m )\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:490\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    489\u001b[0m         new_e \u001b[38;5;241m=\u001b[39m _wrap_proxy_error(new_e, conn\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mscheme)\n\u001b[1;32m--> 490\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[0;32m    492\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 466\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_conn(conn)\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:1095\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m-> 1095\u001b[0m     conn\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:693\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    692\u001b[0m sock: socket\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m|\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLSocket\n\u001b[1;32m--> 693\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_conn()\n\u001b[0;32m    694\u001b[0m server_hostname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connection.py:206\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x00000260070CA930>: Failed to resolve 'oauth2.googleapis.com' ([Errno 11001] getaddrinfo failed)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    668\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    669\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    670\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    671\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    672\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    673\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    674\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    675\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    676\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    677\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    678\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    679\u001b[0m     )\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:873\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    870\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[0;32m    872\u001b[0m     )\n\u001b[1;32m--> 873\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    874\u001b[0m         method,\n\u001b[0;32m    875\u001b[0m         url,\n\u001b[0;32m    876\u001b[0m         body,\n\u001b[0;32m    877\u001b[0m         headers,\n\u001b[0;32m    878\u001b[0m         retries,\n\u001b[0;32m    879\u001b[0m         redirect,\n\u001b[0;32m    880\u001b[0m         assert_same_host,\n\u001b[0;32m    881\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    882\u001b[0m         pool_timeout\u001b[38;5;241m=\u001b[39mpool_timeout,\n\u001b[0;32m    883\u001b[0m         release_conn\u001b[38;5;241m=\u001b[39mrelease_conn,\n\u001b[0;32m    884\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    885\u001b[0m         body_pos\u001b[38;5;241m=\u001b[39mbody_pos,\n\u001b[0;32m    886\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    887\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    888\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    889\u001b[0m     )\n\u001b[0;32m    891\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:873\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    870\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[0;32m    872\u001b[0m     )\n\u001b[1;32m--> 873\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    874\u001b[0m         method,\n\u001b[0;32m    875\u001b[0m         url,\n\u001b[0;32m    876\u001b[0m         body,\n\u001b[0;32m    877\u001b[0m         headers,\n\u001b[0;32m    878\u001b[0m         retries,\n\u001b[0;32m    879\u001b[0m         redirect,\n\u001b[0;32m    880\u001b[0m         assert_same_host,\n\u001b[0;32m    881\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    882\u001b[0m         pool_timeout\u001b[38;5;241m=\u001b[39mpool_timeout,\n\u001b[0;32m    883\u001b[0m         release_conn\u001b[38;5;241m=\u001b[39mrelease_conn,\n\u001b[0;32m    884\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    885\u001b[0m         body_pos\u001b[38;5;241m=\u001b[39mbody_pos,\n\u001b[0;32m    886\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    887\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    888\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    889\u001b[0m     )\n\u001b[0;32m    891\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:873\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    870\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[0;32m    872\u001b[0m     )\n\u001b[1;32m--> 873\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    874\u001b[0m         method,\n\u001b[0;32m    875\u001b[0m         url,\n\u001b[0;32m    876\u001b[0m         body,\n\u001b[0;32m    877\u001b[0m         headers,\n\u001b[0;32m    878\u001b[0m         retries,\n\u001b[0;32m    879\u001b[0m         redirect,\n\u001b[0;32m    880\u001b[0m         assert_same_host,\n\u001b[0;32m    881\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    882\u001b[0m         pool_timeout\u001b[38;5;241m=\u001b[39mpool_timeout,\n\u001b[0;32m    883\u001b[0m         release_conn\u001b[38;5;241m=\u001b[39mrelease_conn,\n\u001b[0;32m    884\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    885\u001b[0m         body_pos\u001b[38;5;241m=\u001b[39mbody_pos,\n\u001b[0;32m    886\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    887\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    888\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    889\u001b[0m     )\n\u001b[0;32m    891\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    841\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 843\u001b[0m retries \u001b[38;5;241m=\u001b[39m retries\u001b[38;5;241m.\u001b[39mincrement(\n\u001b[0;32m    844\u001b[0m     method, url, error\u001b[38;5;241m=\u001b[39mnew_e, _pool\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, _stacktrace\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    845\u001b[0m )\n\u001b[0;32m    846\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\urllib3\\util\\retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[1;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='oauth2.googleapis.com', port=443): Max retries exceeded with url: /token (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000260070CA930>: Failed to resolve 'oauth2.googleapis.com' ([Errno 11001] getaddrinfo failed)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\auth\\transport\\requests.py:186\u001b[0m, in \u001b[0;36mRequest.__call__\u001b[1;34m(self, url, method, body, headers, timeout, **kwargs)\u001b[0m\n\u001b[0;32m    185\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaking request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, method, url)\n\u001b[1;32m--> 186\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    187\u001b[0m     method, url, data\u001b[38;5;241m=\u001b[39mbody, headers\u001b[38;5;241m=\u001b[39mheaders, timeout\u001b[38;5;241m=\u001b[39mtimeout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    188\u001b[0m )\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _Response(response)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\requests\\adapters.py:700\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    698\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='oauth2.googleapis.com', port=443): Max retries exceeded with url: /token (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000260070CA930>: Failed to resolve 'oauth2.googleapis.com' ([Errno 11001] getaddrinfo failed)\"))",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTransportError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m id_pesquisa_aluno \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1GDCAa1fiflDIBnRY9rrdY9ghKwS6DPzxIG9FYGZEsQU\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# === Carregar os dados direto da nuvem\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m df_leads \u001b[38;5;241m=\u001b[39m carregar_aba(id_pesquisa_captacao)\n\u001b[0;32m     36\u001b[0m df_alunos \u001b[38;5;241m=\u001b[39m carregar_aba(id_pesquisa_aluno)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# === Garantir cópias para segurança\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 25\u001b[0m, in \u001b[0;36mcarregar_aba\u001b[1;34m(sheet_id, aba_nome)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcarregar_aba\u001b[39m(sheet_id, aba_nome\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDados\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 25\u001b[0m     planilha \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mopen_by_key(sheet_id)\n\u001b[0;32m     26\u001b[0m     aba \u001b[38;5;241m=\u001b[39m planilha\u001b[38;5;241m.\u001b[39mworksheet(aba_nome)\n\u001b[0;32m     27\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(aba\u001b[38;5;241m.\u001b[39mget_all_records())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gspread\\client.py:168\u001b[0m, in \u001b[0;36mClient.open_by_key\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Opens a spreadsheet specified by `key` (a.k.a Spreadsheet ID).\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03m:param str key: A key of a spreadsheet as it appears in a URL in a browser.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;124;03m>>> gc.open_by_key('0BmgG6nO_6dprdS1MN3d3MkdPa142WFRrdnRRUWl1UFE')\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 168\u001b[0m     spreadsheet \u001b[38;5;241m=\u001b[39m Spreadsheet(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: key})\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m APIError \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ex\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m HTTPStatus\u001b[38;5;241m.\u001b[39mNOT_FOUND:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gspread\\spreadsheet.py:29\u001b[0m, in \u001b[0;36mSpreadsheet.__init__\u001b[1;34m(self, http_client, properties)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m http_client\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_properties \u001b[38;5;241m=\u001b[39m properties\n\u001b[1;32m---> 29\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfetch_sheet_metadata()\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_properties\u001b[38;5;241m.\u001b[39mupdate(metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproperties\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gspread\\spreadsheet.py:230\u001b[0m, in \u001b[0;36mSpreadsheet.fetch_sheet_metadata\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch_sheet_metadata\u001b[39m(\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28mself\u001b[39m, params: Optional[ParamsType] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    220\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Mapping[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m    221\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Similar to :method spreadsheets_get:`gspread.http_client.spreadsheets_get`,\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03m    get the spreadsheet form the API but by default **does not get the cells data**.\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m    It only retrieve the the metadata from the spreadsheet.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03m    :rtype: dict\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mfetch_sheet_metadata(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid, params\u001b[38;5;241m=\u001b[39mparams)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gspread\\http_client.py:305\u001b[0m, in \u001b[0;36mHTTPClient.fetch_sheet_metadata\u001b[1;34m(self, id, params)\u001b[0m\n\u001b[0;32m    301\u001b[0m     params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincludeGridData\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m    303\u001b[0m url \u001b[38;5;241m=\u001b[39m SPREADSHEET_URL \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mid\u001b[39m\n\u001b[1;32m--> 305\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gspread\\http_client.py:114\u001b[0m, in \u001b[0;36mHTTPClient.request\u001b[1;34m(self, method, endpoint, params, data, json, files, headers)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    106\u001b[0m     method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m     headers: Optional[MutableMapping[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    113\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[1;32m--> 114\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    115\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    116\u001b[0m         url\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[0;32m    117\u001b[0m         json\u001b[38;5;241m=\u001b[39mjson,\n\u001b[0;32m    118\u001b[0m         params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    119\u001b[0m         data\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m    120\u001b[0m         files\u001b[38;5;241m=\u001b[39mfiles,\n\u001b[0;32m    121\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    122\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m    123\u001b[0m     )\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mok:\n\u001b[0;32m    126\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\auth\\transport\\requests.py:533\u001b[0m, in \u001b[0;36mAuthorizedSession.request\u001b[1;34m(self, method, url, data, headers, max_allowed_time, timeout, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m remaining_time \u001b[38;5;241m=\u001b[39m max_allowed_time\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TimeoutGuard(remaining_time) \u001b[38;5;28;01mas\u001b[39;00m guard:\n\u001b[1;32m--> 533\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcredentials\u001b[38;5;241m.\u001b[39mbefore_request(auth_request, method, url, request_headers)\n\u001b[0;32m    534\u001b[0m remaining_time \u001b[38;5;241m=\u001b[39m guard\u001b[38;5;241m.\u001b[39mremaining_timeout\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TimeoutGuard(remaining_time) \u001b[38;5;28;01mas\u001b[39;00m guard:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\auth\\credentials.py:239\u001b[0m, in \u001b[0;36mCredentials.before_request\u001b[1;34m(self, request, method, url, headers)\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_blocking_refresh(request)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking_refresh(request)\n\u001b[0;32m    241\u001b[0m metrics\u001b[38;5;241m.\u001b[39madd_metric_header(headers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metric_header_for_usage())\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(headers)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\auth\\credentials.py:202\u001b[0m, in \u001b[0;36mCredentials._blocking_refresh\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_blocking_refresh\u001b[39m(\u001b[38;5;28mself\u001b[39m, request):\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid:\n\u001b[1;32m--> 202\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefresh(request)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\oauth2\\service_account.py:448\u001b[0m, in \u001b[0;36mCredentials.refresh\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    447\u001b[0m     assertion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_authorization_grant_assertion()\n\u001b[1;32m--> 448\u001b[0m     access_token, expiry, _ \u001b[38;5;241m=\u001b[39m _client\u001b[38;5;241m.\u001b[39mjwt_grant(\n\u001b[0;32m    449\u001b[0m         request, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token_uri, assertion\n\u001b[0;32m    450\u001b[0m     )\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken \u001b[38;5;241m=\u001b[39m access_token\n\u001b[0;32m    452\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpiry \u001b[38;5;241m=\u001b[39m expiry\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\oauth2\\_client.py:299\u001b[0m, in \u001b[0;36mjwt_grant\u001b[1;34m(request, token_uri, assertion, can_retry)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Implements the JWT Profile for OAuth 2.0 Authorization Grants.\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03mFor more details, see `rfc7523 section 4`_.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;124;03m.. _rfc7523 section 4: https://tools.ietf.org/html/rfc7523#section-4\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    297\u001b[0m body \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massertion\u001b[39m\u001b[38;5;124m\"\u001b[39m: assertion, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrant_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: _JWT_GRANT_TYPE}\n\u001b[1;32m--> 299\u001b[0m response_data \u001b[38;5;241m=\u001b[39m _token_endpoint_request(\n\u001b[0;32m    300\u001b[0m     request,\n\u001b[0;32m    301\u001b[0m     token_uri,\n\u001b[0;32m    302\u001b[0m     body,\n\u001b[0;32m    303\u001b[0m     can_retry\u001b[38;5;241m=\u001b[39mcan_retry,\n\u001b[0;32m    304\u001b[0m     headers\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m    305\u001b[0m         metrics\u001b[38;5;241m.\u001b[39mAPI_CLIENT_HEADER: metrics\u001b[38;5;241m.\u001b[39mtoken_request_access_token_sa_assertion()\n\u001b[0;32m    306\u001b[0m     },\n\u001b[0;32m    307\u001b[0m )\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    310\u001b[0m     access_token \u001b[38;5;241m=\u001b[39m response_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccess_token\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\oauth2\\_client.py:259\u001b[0m, in \u001b[0;36m_token_endpoint_request\u001b[1;34m(request, token_uri, body, access_token, use_json, can_retry, headers, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_token_endpoint_request\u001b[39m(\n\u001b[0;32m    221\u001b[0m     request,\n\u001b[0;32m    222\u001b[0m     token_uri,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    229\u001b[0m ):\n\u001b[0;32m    230\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Makes a request to the OAuth 2.0 authorization server's token endpoint.\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;124;03m            an error.\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 259\u001b[0m     response_status_ok, response_data, retryable_error \u001b[38;5;241m=\u001b[39m _token_endpoint_request_no_throw(\n\u001b[0;32m    260\u001b[0m         request,\n\u001b[0;32m    261\u001b[0m         token_uri,\n\u001b[0;32m    262\u001b[0m         body,\n\u001b[0;32m    263\u001b[0m         access_token\u001b[38;5;241m=\u001b[39maccess_token,\n\u001b[0;32m    264\u001b[0m         use_json\u001b[38;5;241m=\u001b[39muse_json,\n\u001b[0;32m    265\u001b[0m         can_retry\u001b[38;5;241m=\u001b[39mcan_retry,\n\u001b[0;32m    266\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    267\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    268\u001b[0m     )\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response_status_ok:\n\u001b[0;32m    270\u001b[0m         _handle_error_response(response_data, retryable_error)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\oauth2\\_client.py:192\u001b[0m, in \u001b[0;36m_token_endpoint_request_no_throw\u001b[1;34m(request, token_uri, body, access_token, use_json, can_retry, headers, **kwargs)\u001b[0m\n\u001b[0;32m    190\u001b[0m retries \u001b[38;5;241m=\u001b[39m _exponential_backoff\u001b[38;5;241m.\u001b[39mExponentialBackoff()\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m retries:\n\u001b[1;32m--> 192\u001b[0m     response \u001b[38;5;241m=\u001b[39m request(\n\u001b[0;32m    193\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mtoken_uri, headers\u001b[38;5;241m=\u001b[39mheaders_to_use, body\u001b[38;5;241m=\u001b[39mbody, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    194\u001b[0m     )\n\u001b[0;32m    195\u001b[0m     response_body \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    196\u001b[0m         response\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(response\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m    199\u001b[0m     )\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;66;03m# response_body should be a JSON\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\google\\auth\\transport\\requests.py:192\u001b[0m, in \u001b[0;36mRequest.__call__\u001b[1;34m(self, url, method, body, headers, timeout, **kwargs)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m caught_exc:\n\u001b[0;32m    191\u001b[0m     new_exc \u001b[38;5;241m=\u001b[39m exceptions\u001b[38;5;241m.\u001b[39mTransportError(caught_exc)\n\u001b[1;32m--> 192\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m new_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcaught_exc\u001b[39;00m\n",
      "\u001b[1;31mTransportError\u001b[0m: HTTPSConnectionPool(host='oauth2.googleapis.com', port=443): Max retries exceeded with url: /token (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000260070CA930>: Failed to resolve 'oauth2.googleapis.com' ([Errno 11001] getaddrinfo failed)\"))"
     ]
    }
   ],
   "source": [
    "# === Imports\n",
    "import gspread\n",
    "from gspread_dataframe import get_as_dataframe\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# === Carregar variáveis de ambiente\n",
    "load_dotenv()\n",
    "\n",
    "# === Autenticação\n",
    "credenciais_path = os.getenv(\"GOOGLE_CREDENTIALS_PATH\")\n",
    "scopes = [\n",
    "    \"https://www.googleapis.com/auth/spreadsheets\",\n",
    "    \"https://www.googleapis.com/auth/drive\"\n",
    "]\n",
    "creds = service_account.Credentials.from_service_account_file(\n",
    "    credenciais_path,\n",
    "    scopes=scopes\n",
    ")\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "# === Função para carregar direto do Google Sheets\n",
    "def carregar_aba(sheet_id, aba_nome=\"Dados\"):\n",
    "    planilha = client.open_by_key(sheet_id)\n",
    "    aba = planilha.worksheet(aba_nome)\n",
    "    df = pd.DataFrame(aba.get_all_records())\n",
    "    return df\n",
    "\n",
    "# === IDs fixos das suas planilhas\n",
    "id_pesquisa_captacao = \"1ukLwu8SoP0U3uirB6w1Ca3TPEITY50c558xNUUN3kj4\"\n",
    "id_pesquisa_aluno = \"1GDCAa1fiflDIBnRY9rrdY9ghKwS6DPzxIG9FYGZEsQU\"\n",
    "\n",
    "# === Carregar os dados direto da nuvem\n",
    "df_leads = carregar_aba(id_pesquisa_captacao)\n",
    "df_alunos = carregar_aba(id_pesquisa_aluno)\n",
    "\n",
    "# === Garantir cópias para segurança\n",
    "df_leads = df_leads.copy()\n",
    "df_alunos = df_alunos.copy()\n",
    "\n",
    "# === Criar variável target 'comprou'\n",
    "df_leads[\"comprou\"] = df_leads[\"email\"].isin(df_alunos[\"email\"]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221a79c4-aea8-4d83-9346-a0d635a4bfc2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_leads.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d27369-1706-4dca-b36b-f95c8339413b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_leads.drop(columns=['email']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3211e31-5b14-4c6a-ac3a-1875633855ce",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_alunos.drop(columns=['email']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c87dc0-8a27-4c9f-837e-dcba18cae4d5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "colunas_excluir = [\"data\", \"email\"]\n",
    "\n",
    "for col in df_leads.select_dtypes(include=[\"category\", \"object\"]).columns:\n",
    "    if col not in colunas_excluir:\n",
    "        print(f\"\\n====== VARIÁVEL: {col.upper()} ======\\n\")\n",
    "        print(df_leads[col].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecff9fa-14fe-40ad-a5eb-25eba78aa926",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for col in df_alunos.select_dtypes(include=[\"category\", \"object\"]).columns:\n",
    "    if col not in colunas_excluir:\n",
    "        print(f\"\\n====== VARIÁVEL: {col.upper()} ======\\n\")\n",
    "        print(df_alunos[col].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc0bf0b-dba4-4a0e-9e70-b47749412df3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_leads.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff89844-75ca-4e05-9f5b-14e27c37f5cf",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_alunos.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a5bc0d-4c56-45fa-a0ab-c64174df6f17",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gerar_tabela_lift_score_ponderado(df_leads, df_alunos, col, col_lancamento=\"lancamentos\", min_leads=5):\n",
    "    \"\"\"\n",
    "    Calcula lift por categoria com base em lançamentos,\n",
    "    usando médias ponderadas e sem warnings futuros.\n",
    "    \"\"\"\n",
    "\n",
    "    ciclos = pd.Index(df_alunos[col_lancamento].dropna().unique()).union(\n",
    "        df_leads[col_lancamento].dropna().unique()\n",
    "    )\n",
    "\n",
    "    linhas = []\n",
    "\n",
    "    for ciclo in ciclos:\n",
    "        leads_ciclo = df_leads[df_leads[col_lancamento] == ciclo]\n",
    "        alunos_ciclo = df_alunos[df_alunos[col_lancamento] == ciclo]\n",
    "\n",
    "        cont_leads = leads_ciclo[col].value_counts().rename(\"qtd_leads\")\n",
    "        cont_alunos = alunos_ciclo[col].value_counts().rename(\"qtd_alunos\")\n",
    "\n",
    "        total_leads = cont_leads.sum()\n",
    "        total_alunos = cont_alunos.sum()\n",
    "\n",
    "        perc_leads = (cont_leads / total_leads * 100).rename(\"percentual_leads\")\n",
    "        perc_alunos = (cont_alunos / total_alunos * 100).rename(\"percentual_alunos\")\n",
    "\n",
    "        tabela = pd.concat([cont_leads, cont_alunos, perc_leads, perc_alunos], axis=1).fillna(0)\n",
    "        tabela[\"lancamento\"] = ciclo\n",
    "        tabela.reset_index(inplace=True)\n",
    "        tabela.rename(columns={\"index\": col}, inplace=True)\n",
    "\n",
    "        tabela_validas = tabela[tabela[\"qtd_leads\"] >= min_leads].copy()\n",
    "        tabela_validas[\"lift\"] = tabela_validas.apply(\n",
    "            lambda row: row[\"percentual_alunos\"] / row[\"percentual_leads\"] if row[\"percentual_leads\"] > 0 else 0,\n",
    "            axis=1\n",
    "        )\n",
    "        tabela_validas[\"score\"] = (tabela_validas[\"lift\"] * tabela_validas[\"qtd_alunos\"]).round(2)\n",
    "        linhas.append(tabela_validas)\n",
    "\n",
    "    df_validado = pd.concat(linhas, ignore_index=True)\n",
    "\n",
    "    # >>> CORREÇÃO AQUI <<<\n",
    "\n",
    "    # Primeiro faz o groupby\n",
    "    agrupado = df_validado.groupby(col, observed=True)\n",
    "\n",
    "    # Depois aplica manualmente\n",
    "    resultados = []\n",
    "    for categoria, grupo in agrupado:\n",
    "        qtd_leads = grupo[\"qtd_leads\"].sum()\n",
    "        qtd_alunos = grupo[\"qtd_alunos\"].sum()\n",
    "        percentual_leads = (grupo[\"percentual_leads\"] * grupo[\"qtd_leads\"]).sum() / grupo[\"qtd_leads\"].sum()\n",
    "        percentual_alunos = (grupo[\"percentual_alunos\"] * grupo[\"qtd_alunos\"]).sum() / grupo[\"qtd_alunos\"].sum() if grupo[\"qtd_alunos\"].sum() > 0 else 0\n",
    "        lift_vals = grupo[\"lift\"].replace([np.inf, -np.inf], 0)\n",
    "        lift = (lift_vals * grupo[\"qtd_alunos\"]).sum() / grupo[\"qtd_alunos\"].sum() if grupo[\"qtd_alunos\"].sum() > 0 else 0\n",
    "\n",
    "        score_vals = grupo[\"score\"].replace([np.inf, -np.inf], 0)\n",
    "        score = score_vals.sum()\n",
    "\n",
    "        resultados.append({\n",
    "            col: categoria,\n",
    "            \"qtd_leads\": round(qtd_leads, 2),\n",
    "            \"qtd_alunos\": round(qtd_alunos, 2),\n",
    "            \"percentual_leads\": round(percentual_leads, 2),\n",
    "            \"percentual_alunos\": round(percentual_alunos, 2),\n",
    "            \"lift\": round(lift, 2),\n",
    "            \"score\": round(score, 2)\n",
    "        })\n",
    "\n",
    "    tabela_final = pd.DataFrame(resultados).set_index(col)\n",
    "\n",
    "    # Anotação\n",
    "    todas_categorias = set(df_leads[col].dropna().unique()).union(set(df_alunos[col].dropna().unique()))\n",
    "    categorias_na_tabela = set(tabela_final.index)\n",
    "    categorias_descartadas = todas_categorias - categorias_na_tabela\n",
    "\n",
    "    # Forçar lift específico para uma categoria\n",
    "    if col == \"renda\" and \"De 1.000 a 3.000\" in tabela_final.index:\n",
    "        tabela_final.loc[\"De 1.000 a 3.000\", \"lift\"] = 0.5\n",
    "        tabela_final.loc[\"De 1.000 a 3.000\", \"score\"] = round(\n",
    "            tabela_final.loc[\"De 1.000 a 3.000\", \"qtd_alunos\"] * 0.5, 2\n",
    "        )\n",
    "\n",
    "    # Forçar lift específico para uma categoria\n",
    "    if col == \"renda\" and \"Estou desempregado(a)\" in tabela_final.index:\n",
    "        tabela_final.loc[\"Estou desempregado(a)\", \"lift\"] = 0.5\n",
    "        tabela_final.loc[\"Estou desempregado(a)\", \"score\"] = round(\n",
    "            tabela_final.loc[\"Estou desempregado(a)\", \"qtd_alunos\"] * 0.5, 2\n",
    "        )\n",
    "\n",
    "    return tabela_final.sort_values(by=\"score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d358ab-d858-46df-b573-256a89b14d08",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lista das colunas a serem avaliadas\n",
    "features_alunos = [\n",
    "    \"renda\",\n",
    "    \"escolaridade\",\n",
    "    \"idade\",\n",
    "    \"nível\",\n",
    "    \"situação profissional\"\n",
    "]\n",
    "\n",
    "for coluna in features_alunos:\n",
    "    print(f\"\\n=== TABELA: {coluna} ===\")\n",
    "    tabela = gerar_tabela_lift_score_ponderado(df_leads, df_alunos, coluna)\n",
    "\n",
    "    # Colunas a somar (ignorando 'lift' e 'score')\n",
    "    colunas_para_somar = [\n",
    "        col for col in tabela.columns\n",
    "        if col not in [\"lift\", \"score\", \"percentual_alunos\", \"percentual_leads\"]\n",
    "        and pd.api.types.is_numeric_dtype(tabela[col])\n",
    "    ]\n",
    "\n",
    "    # Criar linha de total\n",
    "    totais = tabela[colunas_para_somar].sum().to_frame().T\n",
    "    totais.index = [\"Total\"]\n",
    "\n",
    "    # Preencher colunas restantes com vazio ou marcador\n",
    "    for col in tabela.columns:\n",
    "        if col not in totais.columns:\n",
    "            totais[col] = \"-\"\n",
    "\n",
    "    # Reordenar para manter consistência\n",
    "    totais = totais[tabela.columns]\n",
    "\n",
    "    # Concatenar com a tabela original\n",
    "    tabela_com_total = pd.concat([tabela, totais], axis=0)\n",
    "\n",
    "    display(tabela_com_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b9e6e9-9373-404f-9012-9f2f50f2525c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Função auxiliar para listar valores únicos, já aplicando strip\n",
    "def listar_valores_unicos(df, coluna):\n",
    "    return sorted({str(v).strip() for v in df[coluna].dropna().unique()})\n",
    "\n",
    "# Loop pelas variáveis\n",
    "for coluna in features_alunos:\n",
    "    print(f\"\\n\\n=== VARIÁVEL: {coluna.upper()} ===\")\n",
    "\n",
    "    valores_leads = listar_valores_unicos(df_leads, coluna)\n",
    "    valores_alunos = listar_valores_unicos(df_alunos, coluna)\n",
    "\n",
    "    print(\"\\n🎯 Valores em df_leads:\")\n",
    "    for v in valores_leads:\n",
    "        print(f\"- '{v}'\")\n",
    "\n",
    "    print(\"\\n🎯 Valores em df_alunos:\")\n",
    "    for v in valores_alunos:\n",
    "        print(f\"- '{v}'\")\n",
    "\n",
    "    print(\"\\n🚨 Categorias nos leads mas não nos alunos:\")\n",
    "    diferenca_leads = set(valores_leads) - set(valores_alunos)\n",
    "    print(diferenca_leads if diferenca_leads else \"Nenhuma diferença.\")\n",
    "\n",
    "    print(\"\\n🚨 Categorias nos alunos mas não nos leads:\")\n",
    "    diferenca_alunos = set(valores_alunos) - set(valores_leads)\n",
    "    print(diferenca_alunos if diferenca_alunos else \"Nenhuma diferença.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c7f9a5-1e52-47af-9531-945c5716d2f1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gerar_distribuicao_por_categoria(df_leads, df_alunos, col):\n",
    "    \"\"\"\n",
    "    Gera a tabela com contagem e percentual de leads e alunos por categoria, sem lift ou score.\n",
    "\n",
    "    Retorno:\n",
    "    -------\n",
    "    pd.DataFrame com colunas: categoria, qtd_leads, qtd_alunos, percentual_leads, percentual_alunos\n",
    "    \"\"\"\n",
    "    contagem_leads = df_leads[col].value_counts().rename(\"qtd_leads\")\n",
    "    contagem_alunos = df_alunos[col].value_counts().rename(\"qtd_alunos\")\n",
    "\n",
    "    percentual_leads = (contagem_leads / contagem_leads.sum() * 100).round(2).rename(\"percentual_leads\")\n",
    "    percentual_alunos = (contagem_alunos / contagem_alunos.sum() * 100).round(2).rename(\"percentual_alunos\")\n",
    "\n",
    "    tabela = pd.concat([contagem_leads, contagem_alunos, percentual_leads, percentual_alunos], axis=1).fillna(0)\n",
    "    tabela = tabela.astype({\"qtd_leads\": int, \"qtd_alunos\": int})\n",
    "    return tabela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6134ba-a654-4cbf-ad63-6dd61820ca84",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for coluna in features_alunos:\n",
    "    print(f\"\\n====== VARIÁVEL: {coluna.upper()} ======\\n\")\n",
    "\n",
    "    for lanc in sorted(df_leads[\"lancamentos\"].dropna().unique()):\n",
    "        print(f\"\\n--- Lançamento: {lanc} ---\")\n",
    "        \n",
    "        leads_subset = df_leads[df_leads[\"lancamentos\"] == lanc]\n",
    "        alunos_subset = df_alunos[df_alunos[\"lancamentos\"] == lanc]\n",
    "\n",
    "        if not leads_subset.empty and not alunos_subset.empty:\n",
    "            tabela = gerar_distribuicao_por_categoria(leads_subset, alunos_subset, coluna)\n",
    "            display(tabela)\n",
    "        else:\n",
    "            print(\"⚠️ Dados insuficientes para este lançamento.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9196e062-3181-40bf-be00-f7fd1bfbb685",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Gerar o novo score_map com base no lift ponderado\n",
    "score_map = {}\n",
    "\n",
    "for col in features_alunos:\n",
    "    tabela = gerar_tabela_lift_score_ponderado(df_leads, df_alunos, col)\n",
    "    \n",
    "    # Normalizar apenas tirando espaços (NÃO converte para minúsculo)\n",
    "    score_map[col] = {\n",
    "        str(k).strip(): v\n",
    "        for k, v in zip(tabela.index, tabela[\"score\"])\n",
    "    }\n",
    "\n",
    "# 2. Função para calcular o score total (somando os scores das variáveis)\n",
    "def calcular_leadscore_total(row, score_map):\n",
    "    total = 0\n",
    "    for var in score_map:\n",
    "        resposta = str(row.get(var)).strip()  # Agora só strip, sem lower\n",
    "        total += score_map[var].get(resposta, 0)\n",
    "    return total\n",
    "\n",
    "# 3. Aplicar aos leads\n",
    "df_leads[\"leadscore_mapeado\"] = df_leads.apply(lambda row: calcular_leadscore_total(row, score_map), axis=1)\n",
    "df_alunos[\"leadscore_mapeado\"] = df_alunos.apply(lambda row: calcular_leadscore_total(row, score_map), axis=1)\n",
    "\n",
    "# 4. Verificações\n",
    "print(\"LEADS\")\n",
    "print(\"Valores únicos:\", df_leads[\"leadscore_mapeado\"].nunique())\n",
    "print(\"Score mínimo:\", df_leads[\"leadscore_mapeado\"].min())\n",
    "print(\"Score máximo:\", df_leads[\"leadscore_mapeado\"].max())\n",
    "print(\"Top 5 scores mais comuns:\")\n",
    "print(df_leads[\"leadscore_mapeado\"].value_counts().head())\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40 + \"\\n\")\n",
    "\n",
    "print(\"ALUNOS\")\n",
    "print(\"Valores únicos:\", df_alunos[\"leadscore_mapeado\"].nunique())\n",
    "print(\"Score mínimo:\", df_alunos[\"leadscore_mapeado\"].min())\n",
    "print(\"Score máximo:\", df_alunos[\"leadscore_mapeado\"].max())\n",
    "print(\"Top 5 scores mais comuns:\")\n",
    "print(df_alunos[\"leadscore_mapeado\"].value_counts().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6852e506-1dea-452c-bcc6-6829ac4c8695",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop para todas as variáveis\n",
    "for col in features_alunos:\n",
    "    print(f\"\\n====== VARIÁVEL: {col.upper()} ======\\n\")\n",
    "\n",
    "    valores_no_df = sorted(df_leads[col].dropna().unique())\n",
    "    valores_no_map = sorted(score_map.get(col, {}).keys())\n",
    "\n",
    "    print(\"❗Valores no df_leads:\")\n",
    "    for v in valores_no_df:\n",
    "        print(f\"- '{v}'\")\n",
    "\n",
    "    print(\"\\n✅ Valores no score_map:\")\n",
    "    for v in valores_no_map:\n",
    "        print(f\"- '{v}'\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6484b9a8-2624-478d-868c-3aeb2971dc06",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calcular a média do leadscore total (já com lift ponderado como base)\n",
    "media_score = df_alunos[\"leadscore_mapeado\"].mean()\n",
    "\n",
    "# Definir os limites com base em proporções da média\n",
    "limite_a = media_score * 1.10\n",
    "limite_b = media_score * 0.90\n",
    "limite_c = media_score * 0.70\n",
    "limite_d = media_score * 0.50\n",
    "\n",
    "print(f\"Média dos scores: {round(media_score)}\")\n",
    "print(f\"Limite A (>= 110%): {round(limite_a)}\")\n",
    "print(f\"Limite B (>=  90%): {round(limite_b)}\")\n",
    "print(f\"Limite C (>=  70%): {round(limite_c)}\")\n",
    "print(f\"Limite D (>=  50%): {round(limite_d)}\")\n",
    "\n",
    "# Função para classificar com base nos limites\n",
    "def classificar_faixa(score):\n",
    "    if score >= limite_a:\n",
    "        return \"A\"\n",
    "    elif score >= limite_b:\n",
    "        return \"B\"\n",
    "    elif score >= limite_c:\n",
    "        return \"C\"\n",
    "    else:\n",
    "        return \"D\"\n",
    "\n",
    "# Aplicar a classificação\n",
    "df_leads[\"leadscore_faixa\"] = df_leads[\"leadscore_mapeado\"].apply(classificar_faixa)\n",
    "df_alunos[\"leadscore_faixa\"] = df_alunos[\"leadscore_mapeado\"].apply(classificar_faixa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d135d165-e184-492d-ba14-57f95ee288e4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Função auxiliar para comparar duas faixas\n",
    "def comparar_faixas(df, colunas, faixa1, faixa2):\n",
    "    resultados = []\n",
    "    for col in colunas:\n",
    "        dist1 = df[df[\"leadscore_faixa\"] == faixa1][col].value_counts(normalize=True) * 100\n",
    "        dist2 = df[df[\"leadscore_faixa\"] == faixa2][col].value_counts(normalize=True) * 100\n",
    "        todas_categorias = set(dist1.index).union(dist2.index)\n",
    "\n",
    "        for cat in todas_categorias:\n",
    "            pct1 = dist1.get(cat, 0)\n",
    "            pct2 = dist2.get(cat, 0)\n",
    "            diff = round(pct1 - pct2, 2)  # diferença direcionada\n",
    "            resultados.append({\n",
    "                \"faixa_origem\": faixa1,\n",
    "                \"faixa_destino\": faixa2,\n",
    "                \"variavel\": col,\n",
    "                \"categoria\": cat,\n",
    "                f\"% {faixa1}\": round(pct1, 2),\n",
    "                f\"% {faixa2}\": round(pct2, 2),\n",
    "                f\"diferença entre {faixa1} e {faixa2}\": diff\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(resultados).sort_values(by=f\"diferença entre {faixa1} e {faixa2}\", key=abs, ascending=False)\n",
    "\n",
    "# Comparações entre faixas consecutivas\n",
    "comparacao_ab = comparar_faixas(df_leads, features_alunos, \"A\", \"B\")\n",
    "comparacao_bc = comparar_faixas(df_leads, features_alunos, \"B\", \"C\")\n",
    "comparacao_cd = comparar_faixas(df_leads, features_alunos, \"C\", \"D\")\n",
    "\n",
    "# Visualizar os top 15 diferenciais de cada comparação\n",
    "print(\"🟢 A → B\")\n",
    "display(comparacao_ab.head(20))\n",
    "\n",
    "print(\"🟡 B → C\")\n",
    "display(comparacao_bc.head(20))\n",
    "\n",
    "print(\"🔴 C → D\")\n",
    "display(comparacao_cd.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbc1fdf-532b-46da-bae7-a22ffc2d195e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Criar estrutura para armazenar os resultados\n",
    "resumo_faixas = []\n",
    "\n",
    "# Lista de colunas que você deseja analisar\n",
    "cols_to_analyze = features_alunos  # ou defina outra lista\n",
    "\n",
    "for var in cols_to_analyze:\n",
    "    # Tabela cruzada: faixa vs categoria (com percentual por faixa)\n",
    "    dist = pd.crosstab(\n",
    "        df_leads[\"leadscore_faixa\"],\n",
    "        df_leads[var],\n",
    "        normalize=\"index\"\n",
    "    ) * 100\n",
    "\n",
    "    dist = dist.round(2)\n",
    "\n",
    "    # Transformar para formato longo\n",
    "    for faixa in dist.index:\n",
    "        for categoria in dist.columns:\n",
    "            resumo_faixas.append({\n",
    "                \"faixa\": faixa,\n",
    "                \"variavel\": var,\n",
    "                \"categoria\": categoria,\n",
    "                \"percentual (%)\": dist.loc[faixa, categoria]\n",
    "            })\n",
    "\n",
    "# Criar DataFrame final\n",
    "df_resumo_faixas = pd.DataFrame(resumo_faixas)\n",
    "\n",
    "# Pivotar para ter as faixas como colunas\n",
    "df_resumo_pivot = df_resumo_faixas.pivot_table(\n",
    "    index=[\"variavel\", \"categoria\"],\n",
    "    columns=\"faixa\",\n",
    "    values=\"percentual (%)\"\n",
    ").reset_index()\n",
    "\n",
    "# Garantir ordem das colunas\n",
    "df_resumo_pivot = df_resumo_pivot[[\"variavel\", \"categoria\", \"A\", \"B\", \"C\", \"D\"]]\n",
    "\n",
    "# Ordenar por variável e percentual na faixa A\n",
    "df_resumo_pivot = df_resumo_pivot.sort_values(\n",
    "    by=[\"variavel\", \"A\"],\n",
    "    ascending=[True, False]\n",
    ")\n",
    "\n",
    "# Exibir tudo\n",
    "pd.set_option('display.max_rows', None)\n",
    "display(df_resumo_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7d7bf2-d756-4111-bc51-82e6e2ebdd65",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ver a distribuição por faixa com percentual\n",
    "def imprimir_distribuicao_faixa(nome, serie):\n",
    "    total = len(serie)\n",
    "    print(f\"\\n{nome}\")\n",
    "    dist = serie.value_counts().sort_index()\n",
    "    for faixa, qtd in dist.items():\n",
    "        perc = (qtd / total) * 100\n",
    "        print(f\"{faixa}: {qtd} ({perc:.1f}%)\")\n",
    "\n",
    "# Executar\n",
    "print(\"\\nDistribuição por faixa de leadscore:\")\n",
    "imprimir_distribuicao_faixa(\"LEADS\", df_leads[\"leadscore_faixa\"])\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "imprimir_distribuicao_faixa(\"ALUNOS\", df_alunos[\"leadscore_faixa\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72171c8e-1f4b-4f70-bb0a-0455a188fcac",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def imprimir_distribuicao_por_lancamento(df, nome_df):\n",
    "    lancamentos = df[\"lancamentos\"].dropna().unique()\n",
    "    print(f\"\\n==== DISTRIBUIÇÃO POR FAIXA — {nome_df.upper()} ====\\n\")\n",
    "\n",
    "    for lanc in sorted(lancamentos):\n",
    "        subset = df[df[\"lancamentos\"] == lanc]\n",
    "        total = len(subset)\n",
    "        print(f\"Lançamento: {lanc} (n = {total})\")\n",
    "\n",
    "        dist = subset[\"leadscore_faixa\"].value_counts(normalize=True).sort_index() * 100\n",
    "        for faixa, perc in dist.items():\n",
    "            print(f\"  {faixa}: {perc:.1f}%\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "# Aplicar\n",
    "imprimir_distribuicao_por_lancamento(df_leads, \"Leads\")\n",
    "print(\"\\n\")\n",
    "imprimir_distribuicao_por_lancamento(df_alunos, \"Alunos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495007c9-3c94-4537-901d-952fa933e4b9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_comparativo_leads_alunos(df_leads, df_alunos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15bc8e9-fe27-477e-8fa4-73e83ed08976",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Contagem por faixa\n",
    "contagem = (\n",
    "    df_leads.groupby([\"lancamentos\", \"leadscore_faixa\"], observed=True)\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# 2. Percentual por faixa\n",
    "percentual = (\n",
    "    contagem.div(contagem.sum(axis=1), axis=0).round(3) * 100\n",
    ")\n",
    "percentual = percentual.rename(columns={col: f\"{col} (%)\" for col in percentual.columns})\n",
    "\n",
    "# 3. Média do leadscore final\n",
    "media_score = (\n",
    "    df_leads.groupby(\"lancamentos\", observed=True)[\"leadscore_mapeado\"]\n",
    "    .mean()\n",
    "    .round(2)\n",
    ")\n",
    "media_score.name = \"leadscore_medio\"\n",
    "\n",
    "# 4. Combinar tudo\n",
    "tabela_lancamentos = pd.concat([contagem, percentual, media_score], axis=1)\n",
    "\n",
    "# 5. Organizar colunas em pares: A, A (%), B, B (%), ...\n",
    "colunas = []\n",
    "for faixa in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "    colunas.append(faixa)\n",
    "    colunas.append(f\"{faixa} (%)\")\n",
    "colunas.append(\"leadscore_medio\")\n",
    "\n",
    "tabela_lancamentos = tabela_lancamentos[colunas]\n",
    "\n",
    "# 6. Exibir\n",
    "display(tabela_lancamentos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d035dab-75a6-429c-809b-cac8d93dbcc0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Contagem por faixa\n",
    "contagem = (\n",
    "    df_alunos.groupby([\"lancamentos\", \"leadscore_faixa\"], observed=False)\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "# 2. Percentual por faixa\n",
    "percentual = (\n",
    "    contagem.div(contagem.sum(axis=1), axis=0).round(3) * 100\n",
    ")\n",
    "percentual = percentual.rename(columns={col: f\"{col} (%)\" for col in percentual.columns})\n",
    "\n",
    "# 3. Média do leadscore final\n",
    "media_score = (\n",
    "    df_leads.groupby(\"lancamentos\", observed=True)[\"leadscore_mapeado\"]\n",
    "    .mean()\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "media_score.name = \"leadscore_medio\"\n",
    "\n",
    "# 4. Combinar tudo\n",
    "tabela_lancamentos = pd.concat([contagem, percentual, media_score], axis=1)\n",
    "\n",
    "# 5. Organizar colunas em pares: A, A (%), B, B (%), ...\n",
    "colunas = []\n",
    "for faixa in [\"A\", \"B\", \"C\", \"D\"]:\n",
    "    colunas.append(faixa)\n",
    "    colunas.append(f\"{faixa} (%)\")\n",
    "colunas.append(\"leadscore_medio\")\n",
    "\n",
    "tabela_lancamentos = tabela_lancamentos[colunas]\n",
    "\n",
    "# 6. Exibir\n",
    "display(tabela_lancamentos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6003eaed-a28c-41b7-acf6-e09bd1cc408b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_histograma_leadscore(df_leads, limite_a, limite_b, limite_c, limite_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7f035a-9cd5-4df8-b58d-0c91a269216c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Seleciona leads com faixa A ou B\n",
    "leads_quentes = df_leads[df_leads[\"leadscore_faixa\"].isin([\"A\", \"B\"])]\n",
    "total_quentes = leads_quentes.shape[0]\n",
    "\n",
    "# Quantos desses já compraram\n",
    "compraram_quentes = leads_quentes[\"comprou\"].sum()\n",
    "\n",
    "# Resultado\n",
    "print(f\"Leads classificados como A ou B: {total_quentes}\")\n",
    "print(f\"Desses, já compraram: {compraram_quentes}\")\n",
    "print(f\"Ainda não compraram: {total_quentes - compraram_quentes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a2100f-2b15-4653-9c5d-39e997f80abf",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "# --- Garantir datas e filtro base ---\n",
    "df_leads['data'] = pd.to_datetime(df_leads['data'], errors='coerce')\n",
    "data_minima = pd.Timestamp(\"2025-04-30\")\n",
    "\n",
    "df_filtrado = df_leads[\n",
    "    (df_leads['lancamentos'] == 'SSP-L13') &\n",
    "    (df_leads['data'] >= data_minima)\n",
    "].copy()\n",
    "\n",
    "# --- Gerar utm_combo sem utm_source ---\n",
    "colunas_utm_sem_source = [\"utm_campaign\", \"utm_medium\", \"utm_content\"]\n",
    "df_filtrado[\"utm_combo\"] = df_filtrado[colunas_utm_sem_source] \\\n",
    "    .applymap(lambda x: '' if pd.isna(x) or str(x).strip().lower() == \"nan\" else str(x).strip()) \\\n",
    "    .agg(' | '.join, axis=1)\n",
    "\n",
    "# --- Agrupar por utm_source e exibir cada bloco ---\n",
    "utm_sources_unicos = df_filtrado['utm_source'].dropna().unique()\n",
    "\n",
    "for utm_src in sorted(utm_sources_unicos):\n",
    "    df_grupo = df_filtrado[df_filtrado['utm_source'] == utm_src]\n",
    "\n",
    "    # Contagem absoluta\n",
    "    dist = df_grupo.groupby(['utm_combo', 'leadscore_faixa']).size().unstack(fill_value=0)\n",
    "    dist[\"Total\"] = dist.sum(axis=1)\n",
    "\n",
    "    # Percentuais por linha\n",
    "    percent = (dist.drop(columns=\"Total\").T / dist[\"Total\"]).T * 100\n",
    "\n",
    "    # Combinar número e percentual\n",
    "    combinado = dist.drop(columns=\"Total\").astype(str) + \" (\" + percent.round(1).astype(str) + \"%)\"\n",
    "    combinado[\"Total\"] = dist[\"Total\"]\n",
    "\n",
    "    # Linha TOTAL GERAL\n",
    "    soma_col = dist.sum()\n",
    "    linha_total = soma_col.drop(\"Total\").astype(int).astype(str) + \" (\" + \\\n",
    "                  (soma_col.drop(\"Total\") / soma_col[\"Total\"] * 100).round(1).astype(str) + \"%)\"\n",
    "    linha_total[\"Total\"] = int(soma_col[\"Total\"])\n",
    "    combinado.loc[\"TOTAL GERAL\"] = linha_total\n",
    "\n",
    "    # Exibir tabela por grupo\n",
    "    print(f\"\\n=== UTM_SOURCE: {utm_src} ===\")\n",
    "    display(combinado.sort_values(by=\"Total\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2507137-e99d-48ea-9610-659e2a6b33ff",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13780474-eda9-44a3-ae5e-a802b0680eb2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe699ad",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Esse bloco monta um modelo de classificação binária para prever a probabilidade de conversão de leads, com:\n",
    "\n",
    "- Entrada: Perfil e respostas do lead\n",
    "- Saída: Chance de virar aluno\n",
    "- Algoritmo: Gradient Boosting\n",
    "- Métrica usada: ROC AUC\n",
    "\"\"\"\n",
    "\n",
    "features_leads = [\n",
    "    \"renda\",\n",
    "    \"escolaridade\",\n",
    "    \"idade\",\n",
    "    \"nível\",\n",
    "    \"situação profissional\"\n",
    "]\n",
    "\n",
    "# One-hot encoding\n",
    "X = pd.get_dummies(df_leads[features_leads], drop_first=False)\n",
    "y = df_leads[\"comprou\"]\n",
    "\n",
    "# Treino/teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Modelo\n",
    "modelo_class = GradientBoostingClassifier(random_state=42)\n",
    "modelo_class.fit(X_train, y_train)\n",
    "\n",
    "# Avaliação\n",
    "roc_auc = roc_auc_score(y_test, modelo_class.predict_proba(X_test)[:, 1])\n",
    "print(f\"ROC AUC (possível comprador): {roc_auc:.3f}\")\n",
    "\n",
    "# (Opcional) Score de probabilidade\n",
    "df_leads[\"probabilidade_conversao_modelo\"] = modelo_class.predict_proba(X)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daaeb7b-b4d9-4445-902f-b3afabf327c1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Certifique-se de ter X_test, y_test e modelo treinado\n",
    "probs = modelo_class.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
    "\n",
    "# Plotar curva ROC\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc_score(y_test, probs):.2f})\", color=cores[2])\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=cores[0])\n",
    "plt.xlabel(\"Falso Positivo Rate\")\n",
    "plt.ylabel(\"Verdadeiro Positivo Rate\")\n",
    "plt.title(\"Curva ROC\")\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# Avaliação para thresholds de 0.0 até 1.0\n",
    "thresholds_to_test = np.linspace(0, 1, 50)\n",
    "metrics = []\n",
    "\n",
    "for thresh in thresholds_to_test:\n",
    "    y_pred = (probs >= thresh).astype(int)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary', zero_division=0)\n",
    "    metrics.append((thresh, precision, recall, f1))\n",
    "\n",
    "# Mostrar tabela de avaliação\n",
    "df_thresholds = pd.DataFrame(metrics, columns=[\"Threshold\", \"Precision\", \"Recall\", \"F1 Score\"])\n",
    "df_thresholds[\"F1 Score\"] = df_thresholds[\"F1 Score\"].round(3)\n",
    "df_thresholds[\"Precision\"] = df_thresholds[\"Precision\"].round(3)\n",
    "df_thresholds[\"Recall\"] = df_thresholds[\"Recall\"].round(3)\n",
    "df_thresholds.sort_values(\"F1 Score\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeb3438-09c5-4fab-8303-67de75ac9b80",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Você poderá usar esse modelo para prever o score mesmo para leads que não constam no score_map ou para variáveis novas.\n",
    "Também ajuda a testar consistência entre o modelo treinado e o score heurístico que você aplicou com base nos lifts.\n",
    "\"\"\"\n",
    "\n",
    "# Preparo dos dados\n",
    "X_reg = pd.get_dummies(df_alunos[features_alunos], drop_first=False)\n",
    "y_reg = df_alunos[\"leadscore_mapeado\"]\n",
    "\n",
    "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X_reg, y_reg, random_state=42)\n",
    "\n",
    "# Modelo de regressão\n",
    "modelo_reg = GradientBoostingRegressor(random_state=42)\n",
    "modelo_reg.fit(X_train_r, y_train_r)\n",
    "\n",
    "# Avaliação\n",
    "y_pred_r = modelo_reg.predict(X_test_r)\n",
    "mae = mean_absolute_error(y_test_r, y_pred_r)\n",
    "r2 = r2_score(y_test_r, y_pred_r)\n",
    "mae_pct = (mae / y_test_r.mean()) * 100\n",
    "\n",
    "print(f\"R² (leadscore_mapeado): {r2:.2f}\")\n",
    "print(f\"Erro médio absoluto (MAE): {mae:.2f}\")\n",
    "print(f\"MAE percentual: {mae_pct:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06130c35-8f2f-4f03-8b38-ac828afa59e4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Previsão nos leads\n",
    "X_leads = pd.get_dummies(df_leads[features_alunos], drop_first=False)\n",
    "X_leads = X_leads.reindex(columns=X_reg.columns, fill_value=0)\n",
    "\n",
    "# Gerar leadscore estimado contínuo\n",
    "df_leads[\"leadscore_regressao\"] = modelo_reg.predict(X_leads).round(2)\n",
    "\n",
    "# Visualizar os top leads por score estimado\n",
    "df_leads.drop(columns=['email']).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89af3f8-9cc3-4845-bbc3-d104b2acdc93",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Validar se o modelo de regressão estima bem as faixas reais dos alunos\n",
    "\n",
    "# 1. Copiar info de score real e faixa real\n",
    "df_avaliacao = df_alunos[[\"email\", \"leadscore_mapeado\", \"leadscore_faixa\"]].copy()\n",
    "\n",
    "# 2. Gerar os X para regressão (features iguais às usadas no modelo)\n",
    "X_alunos = pd.get_dummies(df_alunos[features_alunos], drop_first=False)\n",
    "X_alunos = X_alunos.reindex(columns=X_reg.columns, fill_value=0)\n",
    "\n",
    "# 3. Aplicar o modelo de regressão\n",
    "df_avaliacao[\"leadscore_regressao\"] = modelo_reg.predict(X_alunos)\n",
    "\n",
    "# 4. Classificar usando a função baseada na média dos alunos\n",
    "df_avaliacao[\"faixa_regressao\"] = df_avaliacao[\"leadscore_regressao\"].apply(classificar_faixa)\n",
    "\n",
    "# 5. Ver a matriz de confusão real x predita\n",
    "comparativo = df_avaliacao.groupby([\"leadscore_faixa\", \"faixa_regressao\"]).size().unstack(fill_value=0)\n",
    "\n",
    "display(comparativo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18a5aec-5ed0-417f-9f2a-80a53b7cd485",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Juntar as bases\n",
    "comparacao = df_leads.merge(\n",
    "    df_alunos,\n",
    "    on=\"email\",\n",
    "    suffixes=(\"_lead\", \"_aluno\")\n",
    ")\n",
    "\n",
    "# Verificar onde as respostas foram diferentes\n",
    "mudancas = {}\n",
    "for var in features_alunos:\n",
    "    col_lead = f\"{var}_lead\"\n",
    "    col_aluno = f\"{var}_aluno\"\n",
    "    \n",
    "    if col_lead in comparacao.columns and col_aluno in comparacao.columns:\n",
    "        # Convertendo para string para evitar erro de comparação entre categoricals com categorias diferentes\n",
    "        diferentes = (comparacao[col_lead].astype(str) != comparacao[col_aluno].astype(str)).sum()\n",
    "        total = comparacao.shape[0]\n",
    "        mudancas[var] = round((diferentes / total) * 100, 1)\n",
    "    else:\n",
    "        mudancas[var] = None  # Ou 0, ou continue\n",
    "\n",
    "# Transformar em DataFrame para exibição\n",
    "df_mudancas = pd.DataFrame.from_dict(mudancas, orient=\"index\", columns=[\"% Respostas Diferentes\"])\n",
    "df_mudancas = df_mudancas.dropna().sort_values(by=\"% Respostas Diferentes\", ascending=False)\n",
    "\n",
    "df_mudancas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b394542-5a0b-408a-80be-31c8a92a8de5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Para cada variável comum, mostrar diferenças\n",
    "for var in features_alunos:\n",
    "    col_lead = f\"{var}_lead\"\n",
    "    col_aluno = f\"{var}_aluno\"\n",
    "    \n",
    "    if col_lead in comparacao.columns and col_aluno in comparacao.columns:\n",
    "        # Comparação como string para evitar erros com categorias\n",
    "        diferentes = comparacao[comparacao[col_lead].astype(str) != comparacao[col_aluno].astype(str)]\n",
    "        total = comparacao.shape[0]\n",
    "\n",
    "        print(f\"\\n🔍 {len(diferentes)} de {total} respostas são diferentes em '{var}'\")\n",
    "        display(diferentes[[\"email\", col_lead, col_aluno]].head(10))\n",
    "    else:\n",
    "        print(f\"\\n⚠️ Coluna '{var}' não encontrada em ambos os DataFrames.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417c01a4-5124-4e5f-863a-da1f1ae56a3a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Esse modelo é o coração da previsão de probabilidade de conversão. E ele tem duas grandes vantagens:\n",
    "\n",
    "1. Calibração com pesos → ajuda a lidar com o desbalanceamento real\n",
    "2. Integração com o score → junta perfil sociodemográfico + quão parecido o lead é com os alunos\n",
    "\"\"\"\n",
    "\n",
    "# Preparar X e y\n",
    "X = pd.get_dummies(df_leads[features_leads], drop_first=False)\n",
    "X[\"leadscore_mapeado\"] = df_leads[\"leadscore_mapeado\"]  # Adiciona o score final como feature\n",
    "y = df_leads[\"comprou\"]\n",
    "\n",
    "# Treino/teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "sample_weight = compute_sample_weight(class_weight=\"balanced\", y=y_train)\n",
    "\n",
    "modelo_calibrado = GradientBoostingClassifier(random_state=42)\n",
    "modelo_calibrado.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "\n",
    "# Score calibrado com base nas duas fontes\n",
    "df_leads[\"probabilidade_conversao_modelo\"] = modelo_calibrado.predict_proba(X)[:, 1]\n",
    "\n",
    "# Avaliação\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, modelo_calibrado.predict_proba(X_test)[:, 1]))\n",
    "print(classification_report(y_test, modelo_calibrado.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e084e1b-0f53-4482-b7be-bafc2e22eb97",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inicializar o scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalizar os dois componentes separadamente\n",
    "leadscore_normalizado = scaler.fit_transform(df_leads[[\"leadscore_regressao\"]]).flatten()\n",
    "probabilidade_normalizada = scaler.fit_transform(df_leads[[\"probabilidade_conversao_modelo\"]]).flatten()\n",
    "\n",
    "# Pesos (ajustáveis)\n",
    "peso_score = 0.5\n",
    "peso_prob = 0.5\n",
    "\n",
    "# Calcular o score híbrido como média ponderada dos valores normalizados\n",
    "df_leads[\"score_hibrido\"] = (\n",
    "    peso_score * leadscore_normalizado +\n",
    "    peso_prob * probabilidade_normalizada\n",
    ").round(4)\n",
    "\n",
    "# Opcional: ordenar para inspeção\n",
    "df_leads = df_leads.sort_values(\"score_hibrido\", ascending=False)\n",
    "\n",
    "# Exibir os 10 primeiros leads rankeados\n",
    "display(df_leads[[\"email\", \"leadscore_regressao\", \"leadscore_faixa\", \"probabilidade_conversao_modelo\", \"score_hibrido\"]].drop(columns=['email']).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629ba879-aca2-4ca5-9da5-f1f330e7602a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Base de referência: score_hibrido dos ALUNOS\n",
    "media_hibrido_alunos = df_leads[df_leads[\"comprou\"] == 1][\"score_hibrido\"].mean()\n",
    "\n",
    "# Limiares baseados na média dos alunos\n",
    "limite_A = media_hibrido_alunos * 1.10\n",
    "limite_B = media_hibrido_alunos * 0.90\n",
    "limite_C = media_hibrido_alunos * 0.70\n",
    "\n",
    "# Função de classificação\n",
    "def classificar_faixa_hibrida(score):\n",
    "    if score >= limite_A:\n",
    "        return \"A\"\n",
    "    elif score >= limite_B:\n",
    "        return \"B\"\n",
    "    elif score >= limite_C:\n",
    "        return \"C\"\n",
    "    else:\n",
    "        return \"D\"\n",
    "\n",
    "# Aplicar a classificação\n",
    "df_leads[\"faixa_score_hibrido\"] = df_leads[\"score_hibrido\"].apply(classificar_faixa_hibrida)\n",
    "\n",
    "# Plotar histograma do score_hibrido\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df_leads[\"score_hibrido\"], bins=30, color=\"mediumseagreen\", edgecolor=\"black\", alpha=0.75)\n",
    "plt.axvline(media_hibrido_alunos, color=\"red\", linestyle=\"--\", label=\"Média (Alunos)\")\n",
    "plt.axvline(limite_A, color=\"green\", linestyle=\"--\", label=\"Limite A\")\n",
    "plt.axvline(limite_B, color=\"orange\", linestyle=\"--\", label=\"Limite B\")\n",
    "plt.axvline(limite_C, color=\"blue\", linestyle=\"--\", label=\"Limite C\")\n",
    "\n",
    "plt.title(\"Distribuição do Score Híbrido\")\n",
    "plt.xlabel(\"Score Híbrido (0 a 1)\")\n",
    "plt.ylabel(\"Quantidade de Leads\")\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Gerar tabela de faixas com base na nova coluna faixa_score_hibrido\n",
    "tabela_faixas_hibrido = (\n",
    "    df_leads\n",
    "    .groupby(\"faixa_score_hibrido\")[\"comprou\"]\n",
    "    .agg([\"count\", \"sum\", \"mean\"])\n",
    "    .rename(columns={\n",
    "        \"count\": \"Total Leads\", \n",
    "        \"sum\": \"Alunos\", \n",
    "        \"mean\": \"Taxa de Conversão\"\n",
    "    })\n",
    "    .sort_index()\n",
    "    .round(4)\n",
    ")\n",
    "\n",
    "# Formatar a taxa de conversão como string percentual\n",
    "tabela_faixas_hibrido[\"Taxa de Conversão (%)\"] = (\n",
    "    tabela_faixas_hibrido[\"Taxa de Conversão\"] * 100\n",
    ").round(1).astype(str) + \"%\"\n",
    "\n",
    "# Opcional: remover coluna bruta\n",
    "tabela_faixas_hibrido = tabela_faixas_hibrido.drop(columns=\"Taxa de Conversão\")\n",
    "\n",
    "# Exibir resultado\n",
    "display(tabela_faixas_hibrido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af571181-b41f-4b48-9bc1-3ca67966def5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calcular média do score híbrido por lançamento\n",
    "media_hibrido_por_lancamento = (\n",
    "    df_leads.groupby(\"lancamentos\", observed=True)[\"score_hibrido\"]\n",
    "    .mean()\n",
    "    .round(2)\n",
    "    .reset_index()\n",
    "    .rename(columns={\"score_hibrido\": \"Probabilidade Relativa (0 - 1)\"})\n",
    ")\n",
    "\n",
    "display(media_hibrido_por_lancamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae8f9f4-afec-4c81-b684-38c64db2afdc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Agrupamento por lançamento\n",
    "tabela_lancamentos_avancada = df_leads.groupby(\"lancamentos\", observed=True).agg(\n",
    "    total_leads=(\"email\", \"count\"),\n",
    "    compras_reais=(\"comprou\", \"sum\"),\n",
    "    score_hibrido_medio=(\"score_hibrido\", \"mean\"),\n",
    "    probabilidade_media_modelo=(\"probabilidade_conversao_modelo\", \"mean\"),\n",
    "    taxa_real_conversao=(\"comprou\", \"mean\")  # 0 = não comprou, 1 = comprou\n",
    ").reset_index()\n",
    "\n",
    "# Formatando colunas\n",
    "tabela_lancamentos_avancada[\"score_hibrido_medio\"] = tabela_lancamentos_avancada[\"score_hibrido_medio\"].round(2)\n",
    "tabela_lancamentos_avancada[\"probabilidade_media_modelo\"] = (tabela_lancamentos_avancada[\"probabilidade_media_modelo\"]).round(2)\n",
    "tabela_lancamentos_avancada[\"taxa_real_conversao (%)\"] = (tabela_lancamentos_avancada[\"taxa_real_conversao\"] * 100).round(2)\n",
    "\n",
    "# Selecionar colunas finais\n",
    "tabela_lancamentos_avancada = tabela_lancamentos_avancada[[\n",
    "    \"lancamentos\",\n",
    "    \"total_leads\",\n",
    "    \"compras_reais\",\n",
    "    \"taxa_real_conversao (%)\",\n",
    "    \"probabilidade_media_modelo\",\n",
    "    \"score_hibrido_medio\"\n",
    "]]\n",
    "\n",
    "# Exibir resultado\n",
    "display(tabela_lancamentos_avancada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562dcd02-bc63-4d2a-b8d0-899a41befd48",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_probabilidade_conversao_vs_score(df_leads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe39f89-c70f-4f1f-8f32-66ac786f4df8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gerar a tabela cruzada real x predita com leads que viraram alunos\n",
    "df_compare = df_leads[df_leads[\"email\"].isin(df_alunos[\"email\"])][[\"email\", \"faixa_score_hibrido\"]]\n",
    "df_compare = df_compare.merge(\n",
    "    df_alunos[[\"email\", \"leadscore_faixa\"]],\n",
    "    on=\"email\"\n",
    ")\n",
    "\n",
    "# Tabela cruzada: faixa real (aluno) vs faixa predita (modelo híbrido)\n",
    "comparativo_faixa = pd.crosstab(df_compare[\"leadscore_faixa\"], df_compare[\"faixa_score_hibrido\"])\n",
    "\n",
    "# Calcular Cramér’s V com essa matriz\n",
    "def cramers_v(confusion_matrix):\n",
    "    chi2, _, _, _ = chi2_contingency(confusion_matrix)\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    r, k = confusion_matrix.shape\n",
    "    return np.sqrt(chi2 / (n * (min(r, k) - 1)))\n",
    "\n",
    "# Calcular e imprimir\n",
    "cramers_v_score = cramers_v(comparativo_faixa)\n",
    "print(f\"Cramér’s V: {cramers_v_score:.3f} (0 = sem associação, 1 = associação perfeita)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6c10f0-88dd-437b-a3eb-f750062f0c4b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Caminho direto para a pasta 'modelos'\n",
    "path_modelos = Path(\"C:/Users/Camilo_Bica/data_science/consultoria/acelerador_petrobras/modelos\")\n",
    "path_modelos.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Média dos compradores (referência para benchmarking)\n",
    "media_compradores = df_alunos[\"leadscore_mapeado\"].mean()\n",
    "\n",
    "# 1. Modelo de regressão para prever leadscore\n",
    "joblib.dump(modelo_reg, path_modelos / \"modelo_regressao_leadscore_total.pkl\")\n",
    "joblib.dump(X_reg.columns.tolist(), path_modelos / \"colunas_regressao.pkl\")\n",
    "\n",
    "# 2. Limites para classificar em faixas A, B, C, D\n",
    "limites = {\n",
    "    \"media_compradores\": media_compradores,\n",
    "    \"limite_a\": limite_a,\n",
    "    \"limite_b\": limite_b,\n",
    "    \"limite_c\": limite_c,\n",
    "    \"limite_d\": limite_d\n",
    "}\n",
    "joblib.dump(limites, path_modelos / \"limites_faixa.pkl\")\n",
    "\n",
    "joblib.dump(score_map, path_modelos / \"score_map.pkl\")\n",
    "\n",
    "# 3. Modelo de classificação calibrado para prever conversão\n",
    "joblib.dump(modelo_calibrado, path_modelos / \"modelo_conversao_calibrado.pkl\")\n",
    "joblib.dump(X.columns.tolist(), path_modelos / \"colunas_modelo_conversao_calibrado.pkl\")\n",
    "\n",
    "tabelas_lift = {}\n",
    "for feature in features_alunos:\n",
    "    tabela = gerar_tabela_lift_score_ponderado(df_leads, df_alunos, feature)\n",
    "    tabelas_lift[feature] = tabela\n",
    "\n",
    "joblib.dump(tabelas_lift, path_modelos / \"tabelas_lift.pkl\")\n",
    "\n",
    "# 4. Confirmação\n",
    "print(\"✅ Modelos e arquivos de configuração exportados para:\", path_modelos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bec90d-a797-4221-a9ff-d39fdf1afd4f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gspread_dataframe import set_with_dataframe\n",
    "from datetime import datetime\n",
    "\n",
    "# Carrega as variáveis de ambiente\n",
    "load_dotenv()\n",
    "\n",
    "# Pega o caminho de forma segura\n",
    "credenciais_path = os.getenv(\"GOOGLE_CREDENTIALS_PATH\")\n",
    "\n",
    "# === Escopos de acesso ===\n",
    "scopes = [\n",
    "    \"https://www.googleapis.com/auth/spreadsheets\",\n",
    "    \"https://www.googleapis.com/auth/drive\"\n",
    "]\n",
    "\n",
    "# === Autenticar ===\n",
    "creds = service_account.Credentials.from_service_account_file(\n",
    "    credenciais_path,\n",
    "    scopes=scopes\n",
    ")\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "# === Função para criar nova planilha e carregar dados ===\n",
    "def criar_planilha_e_enviar(df, nome_base):\n",
    "    nome_final = nome_base\n",
    "    \n",
    "    # Cria nova planilha\n",
    "    planilha = client.create(nome_final)\n",
    "    \n",
    "    # Compartilha com seu e-mail pessoal (aqui você coloca o seu)\n",
    "    planilha.share('camilobf2@gmail.com', perm_type='user', role='writer')  # <<< Trocar pelo seu email do Gmail\n",
    "\n",
    "    # Preenche a primeira aba\n",
    "    aba = planilha.sheet1\n",
    "    aba.update_title(\"Dados\")\n",
    "    set_with_dataframe(aba, df)\n",
    "    \n",
    "    print(f\"✅ Nova planilha criada: {nome_final}\")\n",
    "    print(f\"🔗 Link: https://docs.google.com/spreadsheets/d/{planilha.id}/edit\")\n",
    "\n",
    "# === Geração dos arquivos ===\n",
    "#criar_planilha_e_enviar(df_alunos, \"leadscore_implementado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d2529e-9eb9-4729-9af7-e9cc629a5198",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === Função para atualizar uma planilha existente ===\n",
    "def atualizar_planilha_existente(df: pd.DataFrame, sheet_id: str, aba_nome: str = \"Dados\"):\n",
    "    \"\"\"\n",
    "    Atualiza uma aba específica de uma planilha no Google Sheets.\n",
    "    \n",
    "    - df: DataFrame com os dados que serão enviados\n",
    "    - sheet_id: ID da planilha (o que vem na URL depois de \"/d/\")\n",
    "    - aba_nome: Nome da aba que será atualizada (default: 'Dados')\n",
    "    \"\"\"\n",
    "    try:\n",
    "        planilha = client.open_by_key(sheet_id)\n",
    "        try:\n",
    "            aba = planilha.worksheet(aba_nome)\n",
    "        except gspread.WorksheetNotFound:\n",
    "            aba = planilha.add_worksheet(title=aba_nome, rows=\"1000\", cols=\"20\")\n",
    "        \n",
    "        aba.clear()  # Limpa dados antigos\n",
    "        set_with_dataframe(aba, df)\n",
    "        \n",
    "        print(f\"✅ Planilha atualizada: https://docs.google.com/spreadsheets/d/{sheet_id}/edit\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro ao atualizar a planilha: {e}\")\n",
    "\n",
    "# === IDs das planilhas (FIXOS, preencha certinho aqui) ===\n",
    "id_leads = \"1otpSf30y2iqykxNNiCmN3DjxVDTuHSVZhOpzwi8UBBc\"\n",
    "id_alunos = \"15RQf1wkiafPZftolCIAwcTFE7rv0_zRTB0zDOCkdPRs\"\n",
    "\n",
    "# === Atualizar todas as planilhas ===\n",
    "atualizar_planilha_existente(df_leads, id_leads)\n",
    "atualizar_planilha_existente(df_alunos, id_alunos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ambiente_controlado)",
   "language": "python",
   "name": "ambiente_controlado"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 19.157773,
   "end_time": "2025-05-09T12:02:20.799423",
   "environment_variables": {},
   "exception": true,
   "input_path": "C:\\Users\\Camilo_Bica\\data_science\\consultoria\\acelerador_petrobras\\notebooks\\leadscore.ipynb",
   "output_path": "C:\\Users\\Camilo_Bica\\data_science\\consultoria\\acelerador_petrobras\\notebooks\\leadscore_output.ipynb",
   "parameters": {},
   "start_time": "2025-05-09T12:02:01.641650",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}